{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "geolocator = Nominatim(user_agent=\"my_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataset(file_path):\n",
    "    # Read the Parquet file\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Calculate missing values per column\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(df)) * 100\n",
    "    data_types = df.dtypes  # Get the data types of each column\n",
    "    \n",
    "    # Prepare the summary DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Column': missing_values.index,\n",
    "        'Missing Values': missing_values.values,\n",
    "        'Percentage Missing (%)': missing_percentage.values,\n",
    "        'Data Type': data_types.values  # Add the data types to the summary\n",
    "    })\n",
    "    \n",
    "    # Sort the summary DataFrame by the number of missing values, descending\n",
    "    summary_df = summary_df.sort_values(by='Missing Values', ascending=False)\n",
    "    \n",
    "    # Reset index for neat presentation\n",
    "    summary_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "def summarize_dataframe(df):\n",
    "    # Calculate missing values per column\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(df)) * 100\n",
    "    data_types = df.dtypes  # Get the data types of each column\n",
    "    \n",
    "    # Prepare the summary DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Column': missing_values.index,\n",
    "        'Missing Values': missing_values.values,\n",
    "        'Percentage Missing (%)': missing_percentage.values,\n",
    "        'Data Type': data_types.values  # Add the data types to the summary\n",
    "    })\n",
    "    \n",
    "    # Sort the summary DataFrame by the number of missing values, descending\n",
    "    summary_df = summary_df.sort_values(by='Missing Values', ascending=False)\n",
    "    \n",
    "    # Reset index for neat presentation\n",
    "    summary_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return summary_df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop \n",
    "- PostalCode permanence,CityName permanence,StreetName permanence,HouseNumber permanence, -> These are already in the long and lat value + high missing value\n",
    "- Permanence short name,\tPermanence long name -> can be used as identifier but delete at least one if not both \n",
    "- EventType Firstcall, EventType Trip -> keep only those realted to heart problems: Chest pain, P039 - Cardiac problem (other than thoracic pain)','P019 - Unconscious - syncope', 'P003 - Cardiac arrest','P038 - Person does not answer the call', 'P008 - Patient with defibrillator - pacemaker'\n",
    "- Delete every observation that has a 'abondon reason?' as these people were 'fixed' \n",
    "- drop everything related to location that is not coordiantes \n",
    "\n",
    "Transform T0 -> T9 to a date column for the incident and a time column; this can be used to calculate time to get to person and to see how long it takes for the person to get to the hospital \n",
    "\n",
    "All 'intervention' datasets have simmilar structure so i propose the same as above for all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_interventions_dataset(file_path):\n",
    "    # List of heart disorder related event types these are the events we want to use\n",
    "    heart_disorder_events = [\n",
    "        'P011 - Chest pain',\n",
    "        'P039 - Cardiac problem (other than thoracic pain)',\n",
    "        'P019 - Unconscious - syncope',\n",
    "        'P003 - Cardiac arrest',\n",
    "        'P038 - Person does not answer the call',\n",
    "        'P008 - Patient with defibrillator - pacemaker'\n",
    "    ]\n",
    "\n",
    "    # Columns to drop, these are in my opinion not important or have to many NaN \n",
    "    columns_to_drop = [\n",
    "    'name_destination_hospital',\n",
    "    'postalcode_destination_hospital', 'cityname_destination_hospital',\n",
    "    'streetname_destination_hospital', 'housenumber_destination_hospital',\n",
    "    'eventtype_firstcall',\n",
    "    'permanence_long_name', \n",
    "    'service_name', 'abandon_reason',\n",
    "    'unavailable_time', 't9','permanence_short_name',\n",
    "    'eventlevel_firstcall','eventlevel_trip',\n",
    "    't1confirmed','housenumber_permanence'\n",
    "]\n",
    "\n",
    "    \n",
    "    # Load the dataset for parquet\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    #Make all the intervetion datasets uniform: make lowercase + add _ as spacing\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_', regex=False)\n",
    "    \n",
    "    # Filter based on heart disorders\n",
    "    df = df[df['eventtype_trip'].isin(heart_disorder_events)]\n",
    "    \n",
    "    # Further filtering to remove rows with an abandon reason as these people are okay\n",
    "    # I assumed that the ambulance was not hurrying back to the hospital of to the person \n",
    "    df = df[df['abandon_reason'].isna()]\n",
    "       \n",
    "    # Some columns are in the bxl dataset and not in the interventions dataset of vica versa \n",
    "    # These lines of code check if they are there and than deletes them \n",
    "    if 'province_intervention' in df.columns:\n",
    "        columns_to_drop.append('province_intervention')\n",
    "\n",
    "    if 'intervention_time_(t1reported)' in df.columns:\n",
    "        columns_to_drop.append('intervention_time_(t1reported)')  \n",
    "\n",
    "    if 'departure_time_(t1reported)' in df.columns:\n",
    "        columns_to_drop.append('departure_time_(t1reported)')  \n",
    " \n",
    "\n",
    "    # drop the columns from above \n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    # The date of the intervention is in T0 \n",
    "    # we are extrecting these as they are string based (06JAN22) and creating a new column named 'date'\n",
    "    df['date'] = df['t0'].str.extract('(\\d{2}[A-Z]{3}\\d{2})')[0]\n",
    "    \n",
    "    # The date is is also in culumns t2 - t7 and is redundend \n",
    "    # this deletes this from the indicidual cell  \n",
    "    t2_till_t7 = ['t2','t3', 't4', 't5', 't6', 't7']\n",
    "    for col in t2_till_t7:\n",
    "        # Convert to datetime\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        # Extract only the time part and replace the column\n",
    "        df[col] = df[col].dt.strftime('%H:%M:%S')\n",
    "    #Fixing t0 and t1 as they have string value in the data\n",
    "    date_prefix_pattern = r'\\d{2}[A-Z]{3}\\d{2}:' #this sets the parameters that need to be removerd 01JUN22:\n",
    "    df['t0'] = df['t0'].str.replace(date_prefix_pattern, '', regex=True)\n",
    "    df['t1'] = df['t1'].str.replace(date_prefix_pattern, '', regex=True)\n",
    "\n",
    "    \n",
    "    # Convert 'date' column to the format dd/mm/yy to make it easier to read\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%d%b%y').dt.strftime('%d/%m/%y')\n",
    "    \n",
    "    # The cityname has the name twice like Antwerpen (Antwerpen) -> this makes it just Antwerpen \n",
    "    df['cityname_permanence'] = df['cityname_permanence'].str.replace(r\"\\s*\\([^()]*\\)\", \"\", regex=True)\n",
    "    df['cityname_intervention'] = df['cityname_intervention'].str.replace(r\"\\s*\\([^()]*\\)\", \"\", regex=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Modify 't' columns by slicing off the first 11 characters\n",
    "def slice_time_columns(df, columns):\n",
    "    for col in columns:\n",
    "        if col in df.columns:  \n",
    "            df[col] = df[col].str.slice(start=11)\n",
    "    return df\n",
    "\n",
    "def process_interventions_BXL_dataset(file_path):\n",
    "    # List of heart disorder related event types\n",
    "    heart_disorder_events = [\n",
    "        'P011 - Chest pain',\n",
    "        'P039 - Cardiac problem (other than thoracic pain)',\n",
    "        'P019 - Unconscious - syncope',\n",
    "        'P003 - Cardiac arrest',\n",
    "        'P038 - Person does not answer the call',\n",
    "        'P008 - Patient with defibrillator - pacemaker'\n",
    "    ]\n",
    "\n",
    "    # Columns to drop\n",
    "    columns_to_drop = [\n",
    "    'name_destination_hospital',\n",
    "    'postalcode_destination_hospital', 'cityname_destination_hospital',\n",
    "    'streetname_destination_hospital', 'housenumber_destination_hospital',\n",
    "    'eventtype_firstcall',\n",
    "    'permanence_long_name', \n",
    "    'service_name', 'abandon_reason',\n",
    "    'unavailable_time', 't9','permanence_short_name',\n",
    "    'eventlevel_firstcall','eventlevel_trip',\n",
    "    't1confirmed','housenumber_permanence',\n",
    "    't0_str',\n",
    "    'eventtype_trip'\n",
    "]\n",
    "\n",
    "    \n",
    "    # Load the dataset for parquet\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    #Make all the intervetion datasets uniform: make lowercase + add _ as spacing\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_', regex=False)\n",
    "    \n",
    "    # Filter based on heart disorders\n",
    "    df = df[df['eventtype_trip'].isin(heart_disorder_events)]\n",
    "    \n",
    "    # Further filtering to remove rows with an abandon reason as these people are okay\n",
    "    df = df[df['abandon_reason'].isna()]\n",
    "       \n",
    "    # Drop some columns that are in some datasets but not in all\n",
    "    if 'province_intervention' in df.columns:\n",
    "        columns_to_drop.append('province_intervention')\n",
    "\n",
    "    if 'intervention_time_(t1reported)' in df.columns:\n",
    "        columns_to_drop.append('intervention_time_(t1reported)')  \n",
    "\n",
    "    if 'departure_time_(t1reported)' in df.columns:\n",
    "        columns_to_drop.append('departure_time_(t1reported)')  \n",
    " \n",
    "\n",
    "    \n",
    "    df['t0_str'] = df['t0'].astype(str)\n",
    "    df['date'] = df['t0_str'].str.extract(r'(\\d{4})-(\\d{2})-(\\d{2})').apply(lambda x: f\"{x[2]}/{x[1]}/{x[0][2:]}\", axis=1)\n",
    "    \n",
    "    # Modify the 't' columns \n",
    "    time_columns = ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']\n",
    "    df = slice_time_columns(df, time_columns)\n",
    "    t2_till_t7 = ['t2','t3', 't4', 't5', 't6', 't7']\n",
    "\n",
    "    for col in t2_till_t7:\n",
    "        # Convert to datetime\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        # Extract only the time part and replace the column\n",
    "        df[col] = df[col].dt.strftime('%H:%M:%S')\n",
    "   \n",
    "\n",
    "    \n",
    "    df['cityname_permanence'] = df['cityname_permanence'].str.replace(r\"\\s*\\([^()]*\\)\", \"\", regex=True)\n",
    "    df['cityname_intervention'] = df['cityname_intervention'].str.replace(r\"\\s*\\([^()]*\\)\", \"\", regex=True)\n",
    "\n",
    "    \n",
    "    # drop the rest of the columns \n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def process_interventions_BXL2_dataset(file_path):\n",
    "    # List of heart disorder related event types these are the events we want to use\n",
    "    heart_disorder_events = [\n",
    "        'P026 N02 - ONWEL ZONDER DUIDELIJKE REDEN',\n",
    "        'P008 N03 - PATIËNT MET DEFIBRILLATOR OF PACEMAKER',\n",
    "        'P011 N05 - PIJN OP DE BORST (Chest Pain)',\n",
    "        'P039 N05 - CARDIAAL PROBLEEM (ANDERE DAN PIJN AAN DE BORST)',\n",
    "        'P011 N01 - PIJN OP DE BORST (Chest Pain)',\n",
    "        'P039 N03 - CARDIAAL PROBLEEM (ANDERE DAN PIJN AAN DE BORST)', \n",
    "        'P011 N03 - PIJN OP DE BORST (Chest Pain)',\n",
    "        'P039 N01 - CARDIAAL PROBLEEM (ANDERE DAN PIJN AAN DE BORST)',\n",
    "        'P011 N04 - PIJN OP DE BORST (Chest Pain)',\n",
    "        'P003  N01 - HARTSTILSTAND - DOOD - OVERLEDEN',\n",
    "        'P019 N01 - Bewusteloos - coma - syncope',\n",
    "       'P059 N05 - Duizeligheid - onpasselijk',\n",
    "       'P019 N03 - Bewusteloos - coma - syncope',\n",
    "       'P026 N01 - ONWEL ZONDER DUIDELIJKE REDEN',\n",
    "    ]\n",
    "\n",
    "    # Columns to drop, these are in my opinion not important or have to many NaN \n",
    "    columns_to_drop = [\n",
    "    'description_nl','ic_description_nl',\n",
    "    'permanence_long_name_nl',\n",
    "    'permanence_long_name_fr',\n",
    "    'service_name_nl',\n",
    "    'vector_type_fr',\n",
    "    'abandon_reason_fr',\n",
    "    'permanence_short_name_nl',\n",
    "    'permanence_short_name_fr',\n",
    "    'name_destination_hospital',\n",
    "    'cityname_destination_hospital',\n",
    "    'streetname_destination_hospital', \n",
    "    'housenumber_destination_hospital',\n",
    "    'housenumber_permanence',\n",
    "    'eventtype_and_eventlevel',\n",
    "    'service_name_fr',\n",
    "    'abandon_reason_nl',\n",
    "    'creationtime',\n",
    "    'vector_type_nl'\n",
    "    ]\n",
    "    \n",
    "    # Load the dataset for parquet\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # Some datasets have a different column name structure \n",
    "    # This make all the intervetion datasets uniform: make lowercase + add _ as spacing\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_', regex=False)\n",
    "    \n",
    "    # Filter based on heart disorders (only keep heart related iunterventions)\n",
    "    df = df[df['eventtype_and_eventlevel'].isin(heart_disorder_events)]\n",
    "        \n",
    "    # If an abandomend reason was given these rows are dropped\n",
    "    df = df[df['abandon_reason_nl'].isna()]\n",
    "       \n",
    "    # Extracting the date that is a string from t0 (least missing values)\n",
    "    df['date'] = df['t0'].str.extract('(\\d{2}[A-Z]{3}\\d{2})')[0]\n",
    "    \n",
    "    #Extracting the time for t2 till t7 \n",
    "    t2_till_t7 = ['t2','t3', 't4', 't5', 't6', 't7']\n",
    "\n",
    "    for col in t2_till_t7:\n",
    "        # Convert to datetime\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        # Extract only the time part and replace the column\n",
    "        df[col] = df[col].dt.strftime('%H:%M:%S')\n",
    "\n",
    "    #Fixing t0 and t1 as they have string value in the data\n",
    "    date_prefix_pattern = r'\\d{2}[A-Z]{3}\\d{2}:' #this sets the parameters that need to be removerd 01JUN22:\n",
    "\n",
    "    df['t0'] = df['t0'].str.replace(date_prefix_pattern, '', regex=True)\n",
    "    df['t1'] = df['t1'].str.replace(date_prefix_pattern, '', regex=True)\n",
    "    \n",
    "    #Getting the postalcode + cleaning cityname columns\n",
    "    df['postalcode_intervention'] = df['cityname_intervention'].str.extract(r'(\\d{4})')[0]\n",
    "    df['cityname_intervention'] = df['cityname_intervention'].str.extract(r'\\((.*?)\\)')[0]\n",
    "    df['postalcode_permanence'] = df['cityname_permanence'].str.extract(r'(\\d{4})')[0]\n",
    "    df['cityname_permanence'] = df['cityname_permanence'].str.extract(r'\\((.*?)\\)')[0]\n",
    "    df['vector_type']=df['vector_type_nl']\n",
    "    \n",
    "    #Changing date format \n",
    "    df['date'] = pd.to_datetime(df['date'], format='%d%b%y').dt.strftime('%d/%m/%y')\n",
    "\n",
    "    # drop the rest of the columns \n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "    return df\n",
    "\n",
    "#Function to count duplicate mission id's \n",
    "def count_same_mission(df):\n",
    "    duplicate_mission_ids = df.duplicated(subset='mission_id', keep=False)\n",
    "    return duplicate_mission_ids.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of interventions where a Ambulance and MUG has been sent out to resulting in a dubble registration -> we could combine these but that we have to look at Intervention duration as they are different for Ambulance and Mug. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_intervention_rows(df):\n",
    "    # Helper function to combine vector types, handling None values\n",
    "    def combine_vector_types(x):\n",
    "        # Filter out None values before joining\n",
    "        filtered_x = filter(None, x)\n",
    "        return \" + \".join(sorted(set(filtered_x)))\n",
    "\n",
    "    # Helper function to get the maximum value, assuming the values are numeric or can be converted\n",
    "    def get_max_value(x):\n",
    "        def convert_to_float(v):\n",
    "            try:\n",
    "                # Try to directly convert to float\n",
    "                return float(str(v).replace(',', '.'))\n",
    "            except ValueError:\n",
    "                # Handle time or duration strings\n",
    "                parts = v.split(':')\n",
    "                if len(parts) == 3:\n",
    "                    # Assuming format is HH:MM:SS.SSS\n",
    "                    return int(parts[0]) * 3600 + int(parts[1]) * 60 + float(parts[2])\n",
    "                elif len(parts) == 2:\n",
    "                    # Assuming format is MM:SS.SSS\n",
    "                    return int(parts[0]) * 60 + float(parts[1])\n",
    "                else:\n",
    "                    # Default case if format is unknown\n",
    "                    return 0\n",
    "        # Ensure x is a list with no None values before finding max\n",
    "        non_none_x = [v for v in x if v is not None]\n",
    "        max_value = max(non_none_x, key=lambda v: convert_to_float(v), default=None)\n",
    "        return max_value if isinstance(max_value, float) else str(max_value)\n",
    "\n",
    "    # Define aggregation rules\n",
    "    aggregation_rules = {\n",
    "        'latitude_permanence': 'first',\n",
    "        'longitude_permanence': 'first',\n",
    "        'latitude_intervention': 'first',\n",
    "        'longitude_intervention': 'first',\n",
    "        't0': 'first',\n",
    "        'vector_type': combine_vector_types,\n",
    "        't1': 'first', \n",
    "        'date': 'first',\n",
    "        'streetname_permanence': 'first',\n",
    "        'postalcode_permanence': 'first',\n",
    "        'postalcode_intervention': 'first',\n",
    "        'cityname_intervention': 'first',\n",
    "    }\n",
    "\n",
    "    # Columns for which we need the maximum value\n",
    "    max_columns = ['t2', 't3', 't4', 't5', 't6', 't7', 'intervention_time_(t1reported)', \n",
    "                   'intervention_time_(t1confirmed)', 'waiting_time', 'intervention_duration', \n",
    "                   'departure_time_(t1reported)', 'departure_time_(t1confirmed)', \n",
    "                   'calculated_traveltime_destination', 'calculated_distance_destination',\n",
    "                   'calculated_traveltime_destinatio', 'number_of_transported_persons']\n",
    "\n",
    "    # Add max rules for specific columns\n",
    "    for col in max_columns:\n",
    "        if col in df.columns:\n",
    "            aggregation_rules[col] = get_max_value\n",
    "\n",
    "    # Group by 'mission_id' and apply the aggregation rules\n",
    "    df_combined = df.groupby('mission_id', as_index=False).agg(aggregation_rules)\n",
    "    \n",
    "    return df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u0167610\\AppData\\Local\\Temp\\ipykernel_25408\\2291299236.py:149: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\u0167610\\AppData\\Local\\Temp\\ipykernel_25408\\2291299236.py:149: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\u0167610\\AppData\\Local\\Temp\\ipykernel_25408\\2291299236.py:149: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\u0167610\\AppData\\Local\\Temp\\ipykernel_25408\\2291299236.py:149: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\u0167610\\AppData\\Local\\Temp\\ipykernel_25408\\2291299236.py:149: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\u0167610\\AppData\\Local\\Temp\\ipykernel_25408\\2291299236.py:149: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\u0167610\\AppData\\Local\\Temp\\ipykernel_25408\\2291299236.py:227: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\u0167610\\AppData\\Local\\Temp\\ipykernel_25408\\2291299236.py:227: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\u0167610\\AppData\\Local\\Temp\\ipykernel_25408\\2291299236.py:227: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\u0167610\\AppData\\Local\\Temp\\ipykernel_25408\\2291299236.py:227: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\u0167610\\AppData\\Local\\Temp\\ipykernel_25408\\2291299236.py:227: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\u0167610\\AppData\\Local\\Temp\\ipykernel_25408\\2291299236.py:227: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same mission ID in intervention_1 for 0 rows\n",
      "Same mission ID in intervention_2 for 0 rows\n",
      "Same mission ID in intervention_3 for 0 rows\n",
      "Same mission ID in intervention_bxl for 0 rows\n",
      "Same mission ID in intervention_bxl2 for 0 rows\n"
     ]
    }
   ],
   "source": [
    "file_path_interventions1 = \"Data\\Data/interventions1.parquet\"\n",
    "df_internetions_1 = process_interventions_dataset(file_path_interventions1)\n",
    "file_path_interventions2 = \"Data\\Data/interventions2.parquet\"\n",
    "df_internetions_2 = process_interventions_dataset(file_path_interventions2)\n",
    "file_path_interventions3 = \"Data\\Data/interventions3.parquet\"\n",
    "df_internetions_3 = process_interventions_dataset(file_path_interventions3)\n",
    "file_path_bxl = \"Data\\Data\\interventions_bxl.parquet.gzip\"\n",
    "df_interventions_bxl = process_interventions_BXL_dataset(file_path_bxl)\n",
    "file_path_bxl2 = \"Data\\Data\\interventions_bxl2.parquet.gzip\"\n",
    "df_interventions_bxl2 = process_interventions_BXL2_dataset(file_path_bxl2)\n",
    "df_interventions_1_comb= combine_intervention_rows(df_internetions_1)\n",
    "df_interventions_2_comb= combine_intervention_rows(df_internetions_2)\n",
    "df_interventions_3_comb= combine_intervention_rows(df_internetions_3)\n",
    "df_interventions_bxl= combine_intervention_rows(df_interventions_bxl)\n",
    "df_interventions_bxl2= combine_intervention_rows(df_interventions_bxl2)\n",
    "print(f\"Same mission ID in intervention_1 for {count_same_mission(df_interventions_1_comb)} rows\")\n",
    "print(f\"Same mission ID in intervention_2 for {count_same_mission(df_interventions_2_comb)} rows\")\n",
    "print(f\"Same mission ID in intervention_3 for {count_same_mission(df_interventions_3_comb)} rows\")\n",
    "print(f\"Same mission ID in intervention_bxl for {count_same_mission(df_interventions_bxl)} rows\")\n",
    "print(f\"Same mission ID in intervention_bxl2 for {count_same_mission(df_interventions_bxl2)} rows\")\n",
    "df_combined_interventions = pd.concat([df_interventions_1_comb, df_interventions_2_comb, df_interventions_3_comb,df_interventions_bxl,df_interventions_bxl2], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data imputing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning postal codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_postal_code(value):\n",
    "    # Convert value to string if not None\n",
    "    if value is not None:\n",
    "        value_str = str(value)\n",
    "        # Remove leading 'B' if present\n",
    "        if value_str.startswith('B'):\n",
    "            value_str = value_str[1:]\n",
    "        # Remove trailing '.0' if present\n",
    "        if value_str.endswith('.0'):\n",
    "            value_str = value_str[:-2]\n",
    "        #Delete first 0\n",
    "        if value_str.startswith('0'):\n",
    "            value_str = value_str[1:]\n",
    "        return value_str\n",
    "    return None\n",
    "\n",
    "df_combined_interventions['postalcode_intervention'] = df_combined_interventions['postalcode_intervention'].apply(clean_postal_code)\n",
    "df_combined_interventions['postalcode_permanence'] = df_combined_interventions['postalcode_permanence'].apply(clean_postal_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing intervention coordiantes based on the intervention city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geocoding error for city H√©l√©cine: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=H%E2%88%9A%C2%A9l%E2%88%9A%C2%A9cine&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "Geocoding error for city Fontaine-l'Ev√™que: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Fontaine-l%27Ev%E2%88%9A%E2%84%A2que&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "Geocoding error for city Saint-L√©ger: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Saint-L%E2%88%9A%C2%A9ger&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n"
     ]
    }
   ],
   "source": [
    "def fetch_coordinates_by_city_name(city_name):\n",
    "    try:\n",
    "        # Attempt to fetch location using city name\n",
    "        location = geolocator.geocode(city_name)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except (GeocoderTimedOut, GeocoderServiceError) as e:\n",
    "        print(f\"Geocoding error for city {city_name}: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "# Assuming 'cityname_intervention' has been cleaned as needed\n",
    "unique_city_names_intervention = df_combined_interventions['cityname_intervention'].dropna().unique()\n",
    "\n",
    "# Dictionary to store city name coordinates\n",
    "city_name_coordinates_intervention = {}\n",
    "\n",
    "# Fetch coordinates for each unique city name\n",
    "for city_name in unique_city_names_intervention:\n",
    "    if city_name not in city_name_coordinates_intervention:  # Check if coordinates are already fetched\n",
    "        lat, lon = fetch_coordinates_by_city_name(city_name)\n",
    "        city_name_coordinates_intervention[city_name] = (lat, lon)\n",
    "\n",
    "def apply_city_coordinates(row):\n",
    "    # Update intervention coordinates if missing\n",
    "    city_name_intervention = row['cityname_intervention']\n",
    "    if city_name_intervention in city_name_coordinates_intervention:\n",
    "        city_lat_intervention, city_lon_intervention = city_name_coordinates_intervention[city_name_intervention]\n",
    "        \n",
    "        if pd.isnull(row['latitude_intervention']):\n",
    "            row['latitude_intervention'] = city_lat_intervention\n",
    "        if pd.isnull(row['longitude_intervention']):\n",
    "            row['longitude_intervention'] = city_lon_intervention\n",
    "    \n",
    "    return row  # Ensure you return the modified row\n",
    "\n",
    "# Apply the function to each row of df_combined_interventions\n",
    "df_combined_interventions = df_combined_interventions.apply(apply_city_coordinates, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute the T columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_timestamps(df):\n",
    "    def to_seconds(time_str):\n",
    "        \"\"\"Convert HH:MM:SS or full datetime to total seconds.\"\"\"\n",
    "        if pd.isna(time_str):\n",
    "            return None\n",
    "        # Check if the string includes a date part and extract only the time part if so\n",
    "        if ' ' in time_str:\n",
    "            time_str = time_str.split(' ')[-1]  # Assume the last part is time\n",
    "        # Split the time part into hours, minutes, and seconds\n",
    "        parts = time_str.split(':')\n",
    "        h, m = int(parts[0]), int(parts[1])\n",
    "        s = int(parts[2]) if len(parts) == 3 else 0  # Handle cases without seconds\n",
    "        return h * 3600 + m * 60 + s\n",
    "\n",
    "    def to_hms(seconds):\n",
    "        \"\"\"Convert total seconds back to HH:MM:SS.\"\"\"\n",
    "        if seconds is None:\n",
    "            return None\n",
    "        h = seconds // 3600\n",
    "        m = (seconds % 3600) // 60\n",
    "        s = seconds % 60\n",
    "        return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "    # Iterate over each row\n",
    "    for index, row in df.iterrows():\n",
    "        timestamps = row[['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']]\n",
    "        missing_indices = [i for i, x in enumerate(timestamps) if pd.isna(x)]\n",
    "        if not missing_indices:\n",
    "            continue\n",
    "        \n",
    "        for missing_index in missing_indices:\n",
    "            if missing_index == 0 or missing_index == len(timestamps) - 1:\n",
    "                continue\n",
    "            prev_known_index = next((i for i in range(missing_index - 1, -1, -1) if not pd.isna(timestamps[i])), None)\n",
    "            next_known_index = next((i for i in range(missing_index + 1, len(timestamps)) if not pd.isna(timestamps[i])), None)\n",
    "            \n",
    "            if prev_known_index is None or next_known_index is None:\n",
    "                continue\n",
    "            \n",
    "            start_time = to_seconds(timestamps.iloc[prev_known_index])\n",
    "            end_time = to_seconds(timestamps.iloc[next_known_index])\n",
    "            if start_time is not None and end_time is not None:\n",
    "                interval = (end_time - start_time) // (next_known_index - prev_known_index)\n",
    "                \n",
    "                for i in range(prev_known_index + 1, next_known_index):\n",
    "                    start_time += interval\n",
    "                    df.at[index, f't{i}'] = to_hms(start_time)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_combined_interventions = impute_timestamps(df_combined_interventions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the differences between the timesstamps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def calculate_time_differences(df):\n",
    "    # Define a helper function to convert time strings to datetime objects\n",
    "    def time_to_datetime(t):\n",
    "        if pd.isnull(t):  # Check for None values\n",
    "            return None\n",
    "        try:\n",
    "            return datetime.strptime(t, '%H:%M:%S')\n",
    "        except ValueError:\n",
    "            # Handle cases where conversion fails due to incorrect format\n",
    "            return None\n",
    "\n",
    "    # Iterate over the t columns to calculate differences\n",
    "    for i in range(0, 7):\n",
    "        t_col_current = f't{i}'\n",
    "        t_col_next = f't{i+1}'\n",
    "        \n",
    "        # Check if both columns exist in the DataFrame\n",
    "        if t_col_current in df.columns and t_col_next in df.columns:\n",
    "            # Convert time strings to datetime objects, handling None values\n",
    "            current_times = df[t_col_current].apply(time_to_datetime)\n",
    "            next_times = df[t_col_next].apply(time_to_datetime)\n",
    "            \n",
    "            # Calculate time differences where both current and next times are not None\n",
    "            time_diffs = [(next - current).total_seconds() / 60 if current is not None and next is not None else None\n",
    "                          for current, next in zip(current_times, next_times)]\n",
    "            \n",
    "            # Store the results in a new column\n",
    "            df[f'{t_col_current}_{t_col_next}_diff'] = time_diffs\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "df_combined_interventions = calculate_time_differences(df_combined_interventions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### there are negative values so these have be be changed to the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4503"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_rows_count = sum((df_combined_interventions[col] < 0).sum() for col in [f't{i}-t{i+1} diff in minutes' for i in range(0, 7)] if col in df_combined_interventions.columns)\n",
    "negative_rows_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the newly created time difference columns\n",
    "time_diff_cols = [f't{i}_t{i+1}_diff' for i in range(0, 7)]\n",
    "\n",
    "# Impute missing values, round to full integer, and rename columns\n",
    "for i, col in enumerate(time_diff_cols):\n",
    "    if col in df_combined_interventions.columns:  # Check if the column exists in the DataFrame\n",
    "       \n",
    "        # Rename the column to \"tx-ty diff in minutes\"\n",
    "        new_col_name = f't{i}-t{i+1} diff in minutes'\n",
    "        df_combined_interventions.rename(columns={col: new_col_name}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff_cols_renamed = [f't{i}-t{i+1} diff in minutes' for i in range(0, 7)]\n",
    "\n",
    "# Iterate over each renamed time difference column\n",
    "for col in time_diff_cols_renamed:\n",
    "    if col in df_combined_interventions.columns:\n",
    "        # Calculate the mean excluding negative values\n",
    "        mean_value = df_combined_interventions[col][df_combined_interventions[col] > 0].mean()\n",
    "\n",
    "        # Replace negative values with the calculated mean\n",
    "        df_combined_interventions[col] = df_combined_interventions[col].apply(lambda x: mean_value if x < 0 else x)\n",
    "\n",
    "        # Ensure the column is rounded to full integers\n",
    "        df_combined_interventions[col] = df_combined_interventions[col].round(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_rows_count = sum((df_combined_interventions[col] < 0).sum() for col in [f't{i}-t{i+1} diff in minutes' for i in range(0, 7)] if col in df_combined_interventions.columns)\n",
    "negative_rows_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimpleImputer object with strategy set to 'mean'\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "number_of_transported = df_combined_interventions['number_of_transported_persons'].values.reshape(-1, 1)\n",
    "df_combined_interventions['number_of_transported_persons'] = imputer.fit_transform(number_of_transported)\n",
    "df_combined_interventions['number_of_transported_persons']=round(df_combined_interventions['number_of_transported_persons'],0)\n",
    "df_combined_interventions['t1-t2 diff in minutes']=round(df_combined_interventions['t1-t2 diff in minutes'],0)\n",
    "df_combined_interventions['t2-t3 diff in minutes']=round(df_combined_interventions['t2-t3 diff in minutes'],0)\n",
    "df_combined_interventions['t3-t4 diff in minutes']=round(df_combined_interventions['t3-t4 diff in minutes'],0)\n",
    "df_combined_interventions['t4-t5 diff in minutes']=round(df_combined_interventions['t4-t5 diff in minutes'],0)\n",
    "df_combined_interventions['t5-t6 diff in minutes']=round(df_combined_interventions['t5-t6 diff in minutes'],0)\n",
    "df_combined_interventions['t6-t7 diff in minutes']=round(df_combined_interventions['t6-t7 diff in minutes'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.34028895811989"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_interventions['t1-t2 diff in minutes'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
