{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "geolocator = Nominatim(user_agent=\"my_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataset(file_path):\n",
    "    # Read the Parquet file\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Calculate missing values per column\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(df)) * 100\n",
    "    data_types = df.dtypes  # Get the data types of each column\n",
    "    \n",
    "    # Prepare the summary DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Column': missing_values.index,\n",
    "        'Missing Values': missing_values.values,\n",
    "        'Percentage Missing (%)': missing_percentage.values,\n",
    "        'Data Type': data_types.values  # Add the data types to the summary\n",
    "    })\n",
    "    \n",
    "    # Sort the summary DataFrame by the number of missing values, descending\n",
    "    summary_df = summary_df.sort_values(by='Missing Values', ascending=False)\n",
    "    \n",
    "    # Reset index for neat presentation\n",
    "    summary_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "def summarize_dataframe(df):\n",
    "    # Calculate missing values per column\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(df)) * 100\n",
    "    data_types = df.dtypes  # Get the data types of each column\n",
    "    \n",
    "    # Prepare the summary DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Column': missing_values.index,\n",
    "        'Missing Values': missing_values.values,\n",
    "        'Percentage Missing (%)': missing_percentage.values,\n",
    "        'Data Type': data_types.values  # Add the data types to the summary\n",
    "    })\n",
    "    \n",
    "    # Sort the summary DataFrame by the number of missing values, descending\n",
    "    summary_df = summary_df.sort_values(by='Missing Values', ascending=False)\n",
    "    \n",
    "    # Reset index for neat presentation\n",
    "    summary_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "parquet_files = {\n",
    "    \"aed_locations\": r\"data\\aed_locations.parquet.gzip\",\n",
    "    \"cad9\": r\"data\\cad9.parquet.gzip\",\n",
    "    \"interventions_bxl\": r\"data\\interventions_bxl.parquet.gzip\",\n",
    "    \"ambulance_locations\": r\"data\\ambulance_locations.parquet.gzip\",\n",
    "    \"interventions_bxl\": r\"data\\interventions_bxl.parquet.gzip\",\n",
    "    \"interventions_bxl2\": r\"data\\interventions_bxl2.parquet.gzip\",\n",
    "    \"interventions1\": r\"data\\interventions1.parquet\",\n",
    "    \"interventions2\": r\"data\\interventions2.parquet\",\n",
    "    \"interventions3\": r\"data\\interventions3.parquet\",\n",
    "    \"mug_locations\": r\"data\\mug_locations.parquet.gzip\",\n",
    "    \"pit_locations\": r\"data\\pit_locations.parquet.gzip\"\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop \n",
    "- PostalCode permanence,CityName permanence,StreetName permanence,HouseNumber permanence, -> These are already in the long and lat value + high missing value\n",
    "- Permanence short name,\tPermanence long name -> can be used as identifier but delete at least one if not both \n",
    "- EventType Firstcall, EventType Trip -> keep only those realted to heart problems: Chest pain, P039 - Cardiac problem (other than thoracic pain)','P019 - Unconscious - syncope', 'P003 - Cardiac arrest','P038 - Person does not answer the call', 'P008 - Patient with defibrillator - pacemaker'\n",
    "- Delete every observation that has a 'abondon reason?' as these people were 'fixed' \n",
    "- drop everything related to location that is not coordiantes \n",
    "\n",
    "Transform T0 -> T9 to a date column for the incident and a time column; this can be used to calculate time to get to person and to see how long it takes for the person to get to the hospital \n",
    "\n",
    "All 'intervention' datasets have simmilar structure so i propose the same as above for all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_interventions_dataset(file_path):\n",
    "    # List of heart disorder related event types\n",
    "    heart_disorder_events = [\n",
    "        'P011 - Chest pain',\n",
    "        'P039 - Cardiac problem (other than thoracic pain)',\n",
    "        'P019 - Unconscious - syncope',\n",
    "        'P003 - Cardiac arrest',\n",
    "        'P038 - Person does not answer the call',\n",
    "        'P008 - Patient with defibrillator - pacemaker'\n",
    "    ]\n",
    "\n",
    "    # Columns to drop\n",
    "    columns_to_drop = [\n",
    "    'name_destination_hospital',\n",
    "    'postalcode_destination_hospital', 'cityname_destination_hospital',\n",
    "    'streetname_destination_hospital', 'housenumber_destination_hospital',\n",
    "    'eventtype_firstcall',\n",
    "    'permanence_long_name', \n",
    "    'service_name', 'abandon_reason',\n",
    "    'unavailable_time', 't9','permanence_short_name',\n",
    "    'eventlevel_firstcall','eventlevel_trip',\n",
    "    't1confirmed','housenumber_permanence'\n",
    "]\n",
    "\n",
    "    \n",
    "    # Load the dataset for parquet\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    #Make all the intervetion datasets uniform: make lowercase + add _ as spacing\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_', regex=False)\n",
    "    \n",
    "    # Filter based on heart disorders\n",
    "    df = df[df['eventtype_trip'].isin(heart_disorder_events)]\n",
    "    \n",
    "    # Further filtering to remove rows with an abandon reason as these people are okay\n",
    "    df = df[df['abandon_reason'].isna()]\n",
    "       \n",
    "    # Drop some columns that are in some datasets but not in all\n",
    "    if 'province_intervention' in df.columns:\n",
    "        columns_to_drop.append('province_intervention')\n",
    "\n",
    "    if 'intervention_time_(t1reported)' in df.columns:\n",
    "        columns_to_drop.append('intervention_time_(t1reported)')  \n",
    "\n",
    "    if 'departure_time_(t1reported)' in df.columns:\n",
    "        columns_to_drop.append('departure_time_(t1reported)')  \n",
    " \n",
    "\n",
    "    # drop the rest of the columns \n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    # Extracting the date that is a string from t0 (least missing values)\n",
    "    df['date'] = df['t0'].str.extract('(\\d{2}[A-Z]{3}\\d{2})')[0]\n",
    "    \n",
    "    #Extracting the time for t2 till t7 \n",
    "    t2_till_t7 = ['t2','t3', 't4', 't5', 't6', 't7']\n",
    "\n",
    "    for col in t2_till_t7:\n",
    "        # Convert to datetime\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        # Extract only the time part and replace the column\n",
    "        df[col] = df[col].dt.strftime('%H:%M:%S')\n",
    "\n",
    "    #Fixing t0 and t1 as they have string value in the data\n",
    "    date_prefix_pattern = r'\\d{2}[A-Z]{3}\\d{2}:' #this sets the parameters that need to be removerd 01JUN22:\n",
    "\n",
    "    df['t0'] = df['t0'].str.replace(date_prefix_pattern, '', regex=True)\n",
    "    df['t1'] = df['t1'].str.replace(date_prefix_pattern, '', regex=True)\n",
    "\n",
    "    \n",
    "    # Convert 'date' column to the format dd/mm/yy\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%d%b%y').dt.strftime('%d/%m/%y')\n",
    "    \n",
    "    df['cityname_permanence'] = df['cityname_permanence'].str.replace(r\"\\s*\\([^()]*\\)\", \"\", regex=True)\n",
    "    df['cityname_intervention'] = df['cityname_intervention'].str.replace(r\"\\s*\\([^()]*\\)\", \"\", regex=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def process_interventions_BXL2_dataset(file_path):\n",
    "    # List of heart disorder related event types\n",
    "    heart_disorder_events = [\n",
    "        'P026 N02 - ONWEL ZONDER DUIDELIJKE REDEN',\n",
    "        'P008 N03 - PATIÃ‹NT MET DEFIBRILLATOR OF PACEMAKER',\n",
    "        'P011 N05 - PIJN OP DE BORST (Chest Pain)',\n",
    "        'P039 N05 - CARDIAAL PROBLEEM (ANDERE DAN PIJN AAN DE BORST)',\n",
    "        'P011 N01 - PIJN OP DE BORST (Chest Pain)',\n",
    "        'P039 N03 - CARDIAAL PROBLEEM (ANDERE DAN PIJN AAN DE BORST)', \n",
    "        'P011 N03 - PIJN OP DE BORST (Chest Pain)',\n",
    "        'P039 N01 - CARDIAAL PROBLEEM (ANDERE DAN PIJN AAN DE BORST)',\n",
    "        'P011 N04 - PIJN OP DE BORST (Chest Pain)',\n",
    "        'P003  N01 - HARTSTILSTAND - DOOD - OVERLEDEN',\n",
    "        'P019 N01 - Bewusteloos - coma - syncope',\n",
    "       'P059 N05 - Duizeligheid - onpasselijk',\n",
    "       'P019 N03 - Bewusteloos - coma - syncope',\n",
    "       'P026 N01 - ONWEL ZONDER DUIDELIJKE REDEN',\n",
    "    ]\n",
    "\n",
    "    # Columns to drop\n",
    "    columns_to_drop = [\n",
    "    'description_nl','ic_description_nl',\n",
    "    'permanence_long_name_nl',\n",
    "    'permanence_long_name_fr',\n",
    "    'service_name_nl',\n",
    "    'vector_type_fr',\n",
    "    'abandon_reason_fr',\n",
    "    'permanence_short_name_nl',\n",
    "    'permanence_short_name_fr',\n",
    "    'name_destination_hospital',\n",
    "    'cityname_destination_hospital',\n",
    "    'streetname_destination_hospital', \n",
    "    'housenumber_destination_hospital',\n",
    "    'housenumber_permanence',\n",
    "    'eventtype_and_eventlevel',\n",
    "    'service_name_fr',\n",
    "    'abandon_reason_nl',\n",
    "    'creationtime',\n",
    "    'vector_type_nl'\n",
    "    ]\n",
    "    \n",
    "    # Load the dataset for parquet\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    #Make all the intervetion datasets uniform: make lowercase + add _ as spacing\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_', regex=False)\n",
    "    \n",
    "    #Make all the intervetion datasets uniform: make lowercase + add _ as spacing\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_', regex=False)\n",
    "    \n",
    "    # Filter based on heart disorders\n",
    "    df = df[df['eventtype_and_eventlevel'].isin(heart_disorder_events)]\n",
    "        \n",
    "    # Further filtering to remove rows with an abandon reason as these people are okay\n",
    "    df = df[df['abandon_reason_nl'].isna()]\n",
    "       \n",
    "    # Extracting the date that is a string from t0 (least missing values)\n",
    "    df['date'] = df['t0'].str.extract('(\\d{2}[A-Z]{3}\\d{2})')[0]\n",
    "    \n",
    "    #Extracting the time for t2 till t7 \n",
    "    t2_till_t7 = ['t2','t3', 't4', 't5', 't6', 't7']\n",
    "\n",
    "    for col in t2_till_t7:\n",
    "        # Convert to datetime\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        # Extract only the time part and replace the column\n",
    "        df[col] = df[col].dt.strftime('%H:%M:%S')\n",
    "\n",
    "    #Fixing t0 and t1 as they have string value in the data\n",
    "    date_prefix_pattern = r'\\d{2}[A-Z]{3}\\d{2}:' #this sets the parameters that need to be removerd 01JUN22:\n",
    "\n",
    "    df['t0'] = df['t0'].str.replace(date_prefix_pattern, '', regex=True)\n",
    "    df['t1'] = df['t1'].str.replace(date_prefix_pattern, '', regex=True)\n",
    "    \n",
    "    #Getting the postalcode + cleaning cityname columns\n",
    "    df['postalcode_intervention'] = df['cityname_intervention'].str.extract(r'(\\d{4})')[0]\n",
    "    df['cityname_intervention'] = df['cityname_intervention'].str.extract(r'\\((.*?)\\)')[0]\n",
    "    df['postalcode_permanence'] = df['cityname_permanence'].str.extract(r'(\\d{4})')[0]\n",
    "    df['cityname_permanence'] = df['cityname_permanence'].str.extract(r'\\((.*?)\\)')[0]\n",
    "    df['vector_type']=df['vector_type_nl']\n",
    "    \n",
    "    #Changing date format \n",
    "    df['date'] = pd.to_datetime(df['date'], format='%d%b%y').dt.strftime('%d/%m/%y')\n",
    "\n",
    "    # drop the rest of the columns \n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "    return df\n",
    "\n",
    "#Function to count duplicate mission id's \n",
    "def count_same_mission(df):\n",
    "    duplicate_mission_ids = df.duplicated(subset='mission_id', keep=False)\n",
    "    return duplicate_mission_ids.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_interventions1 = \"data/interventions1.parquet\"\n",
    "df_internetions_1 = process_interventions_dataset(file_path_interventions1)\n",
    "file_path_interventions2 = \"data/interventions2.parquet\"\n",
    "df_internetions_2 = process_interventions_dataset(file_path_interventions2)\n",
    "file_path_interventions3 = \"data/interventions3.parquet\"\n",
    "df_internetions_3 = process_interventions_dataset(file_path_interventions3)\n",
    "\n",
    "file_path_bxl2 = \"data\\interventions_bxl2.parquet.gzip\"\n",
    "df_interventions_bxl2 = process_interventions_BXL2_dataset(file_path_bxl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23        2022-09-06 14:37:30.4641126 +02:00\n",
       "24        2022-09-06 14:38:11.1705396 +02:00\n",
       "28        2022-09-06 14:51:41.9686773 +02:00\n",
       "29        2022-09-06 14:51:41.9686773 +02:00\n",
       "34        2022-09-06 15:01:24.1076499 +02:00\n",
       "                         ...                \n",
       "115559    2023-05-31 19:24:30.8218695 +02:00\n",
       "115563    2023-05-31 19:35:08.2542141 +02:00\n",
       "115567    2023-05-31 19:35:08.2542141 +02:00\n",
       "115623    2023-05-31 22:17:36.0431930 +02:00\n",
       "115641    2023-05-31 23:26:41.8035155 +02:00\n",
       "Name: t0, Length: 13850, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_interventions_BXL_dataset(file_path):\n",
    "    # List of heart disorder related event types\n",
    "    heart_disorder_events = [\n",
    "        'P011 - Chest pain',\n",
    "        'P039 - Cardiac problem (other than thoracic pain)',\n",
    "        'P019 - Unconscious - syncope',\n",
    "        'P003 - Cardiac arrest',\n",
    "        'P038 - Person does not answer the call',\n",
    "        'P008 - Patient with defibrillator - pacemaker'\n",
    "    ]\n",
    "\n",
    "    # Columns to drop\n",
    "    columns_to_drop = [\n",
    "    'name_destination_hospital',\n",
    "    'postalcode_destination_hospital', 'cityname_destination_hospital',\n",
    "    'streetname_destination_hospital', 'housenumber_destination_hospital',\n",
    "    'eventtype_firstcall',\n",
    "    'permanence_long_name', \n",
    "    'service_name', 'abandon_reason',\n",
    "    'unavailable_time', 't9','permanence_short_name',\n",
    "    'eventlevel_firstcall','eventlevel_trip',\n",
    "    't1confirmed','housenumber_permanence',\n",
    "    't0_str',\n",
    "    'eventtype_trip'\n",
    "]\n",
    "\n",
    "    \n",
    "    # Load the dataset for parquet\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    #Make all the intervetion datasets uniform: make lowercase + add _ as spacing\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_', regex=False)\n",
    "    \n",
    "    # Filter based on heart disorders\n",
    "    df = df[df['eventtype_trip'].isin(heart_disorder_events)]\n",
    "    \n",
    "    # Further filtering to remove rows with an abandon reason as these people are okay\n",
    "    df = df[df['abandon_reason'].isna()]\n",
    "       \n",
    "    # Drop some columns that are in some datasets but not in all\n",
    "    if 'province_intervention' in df.columns:\n",
    "        columns_to_drop.append('province_intervention')\n",
    "\n",
    "    if 'intervention_time_(t1reported)' in df.columns:\n",
    "        columns_to_drop.append('intervention_time_(t1reported)')  \n",
    "\n",
    "    if 'departure_time_(t1reported)' in df.columns:\n",
    "        columns_to_drop.append('departure_time_(t1reported)')  \n",
    " \n",
    "\n",
    "    \n",
    "    df['t0_str'] = df['t0'].astype(str)\n",
    "    df['date'] = df['t0_str'].str.extract(r'(\\d{4})-(\\d{2})-(\\d{2})').apply(lambda x: f\"{x[2]}/{x[1]}/{x[0][2:]}\", axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    df['cityname_permanence'] = df['cityname_permanence'].str.replace(r\"\\s*\\([^()]*\\)\", \"\", regex=True)\n",
    "    df['cityname_intervention'] = df['cityname_intervention'].str.replace(r\"\\s*\\([^()]*\\)\", \"\", regex=True)\n",
    "\n",
    "    \n",
    "    # drop the rest of the columns \n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "    return df\n",
    "\n",
    "file_path_bxl = \"data\\interventions_bxl.parquet.gzip\"\n",
    "df_interventions_bxl = process_interventions_BXL_dataset(file_path_bxl)\n",
    "df_interventions_bxl['t0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same mission ID in intervention_1 for 17632 rows\n",
      "Same mission ID in intervention_2 for 17681 rows\n",
      "Same mission ID in intervention_3 for 18254 rows\n",
      "Same mission ID in intervention_bxl for 7042 rows\n",
      "Same mission ID in intervention_bxl for 2024 rows\n"
     ]
    }
   ],
   "source": [
    "print(f\"Same mission ID in intervention_1 for {count_same_mission(df_internetions_1)} rows\")\n",
    "print(f\"Same mission ID in intervention_2 for {count_same_mission(df_internetions_2)} rows\")\n",
    "print(f\"Same mission ID in intervention_3 for {count_same_mission(df_internetions_3)} rows\")\n",
    "print(f\"Same mission ID in intervention_bxl for {count_same_mission(df_interventions_bxl)} rows\")\n",
    "print(f\"Same mission ID in intervention_bxl for {count_same_mission(df_interventions_bxl2)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of interventions where a Ambulance and MUG has been sent out to resulting in a dubble registration -> we could combine these but that we have to look at Intervention duration as they are different for Ambulance and Mug. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_intervention_rows(df):\n",
    "    # Helper function to combine vector types\n",
    "    def combine_vector_types(x):\n",
    "        return \" + \".join(sorted(set(x)))\n",
    "\n",
    "    # Helper function to get the maximum value, assuming the values are numeric or can be converted\n",
    "    def get_max_value(x):\n",
    "        def convert_to_float(v):\n",
    "            try:\n",
    "                # Try to directly convert to float\n",
    "                return float(str(v).replace(',', '.'))\n",
    "            except ValueError:\n",
    "                # Handle time or duration strings\n",
    "                parts = v.split(':')\n",
    "                if len(parts) == 3:\n",
    "                    # Assuming format is HH:MM:SS.SSS\n",
    "                    return int(parts[0]) * 3600 + int(parts[1]) * 60 + float(parts[2])\n",
    "                elif len(parts) == 2:\n",
    "                    # Assuming format is MM:SS.SSS\n",
    "                    return int(parts[0]) * 60 + float(parts[1])\n",
    "                else:\n",
    "                    # Default case if format is unknown\n",
    "                    return 0\n",
    "        max_value = max(x, key=lambda v: convert_to_float(v))\n",
    "        return max_value if isinstance(max_value, float) else str(max_value)\n",
    "\n",
    "    # Define aggregation rules\n",
    "    aggregation_rules = {\n",
    "        'latitude_permanence': 'first',\n",
    "        'longitude_permanence': 'first',\n",
    "        'latitude_intervention': 'first',\n",
    "        'longitude_intervention': 'first',\n",
    "        't0': 'first',\n",
    "        'vector_type': combine_vector_types,\n",
    "        't1': 'first', \n",
    "        'date':'first',\n",
    "        'streetname_permanence':'first',\n",
    "        'postalcode_permanence':'first',\n",
    "        'postalcode_intervention':'first',\n",
    "        'cityname_intervention':'first',\n",
    "    }\n",
    "\n",
    "    # Columns for which we need the maximum value\n",
    "    max_columns = ['t2', 't3', 't4', 't5', 't6', 't7', 'intervention_time_(t1reported)', \n",
    "                   'intervention_time_(t1confirmed)', 'waiting_time', 'intervention_duration', \n",
    "                   'departure_time_(t1reported)', 'departure_time_(t1confirmed)', \n",
    "                   'calculated_traveltime_destination', 'calculated_distance_destination',\n",
    "                   'calculated_traveltime_destinatio', 'number_of_transported_persons']\n",
    "\n",
    "    # Add max rules for specific columns\n",
    "    for col in max_columns:\n",
    "        if col in df.columns:\n",
    "            aggregation_rules[col] = get_max_value\n",
    "\n",
    "    # Group by 'mission_id' and apply the aggregation rules\n",
    "    df_combined = df.groupby('mission_id', as_index=False).agg(aggregation_rules)\n",
    "    \n",
    "    return df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11436\\1231017698.py\u001b[0m in \u001b[0;36mconvert_to_float\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[1;31m# Try to directly convert to float\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'None'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11436\\1644495030.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf_interventions_2_comb\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcombine_intervention_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_internetions_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_interventions_3_comb\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcombine_intervention_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_internetions_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf_interventions_bxl\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcombine_intervention_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_interventions_bxl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdf_interventions_bxl2\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcombine_intervention_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_interventions_bxl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Same mission ID in intervention_1 for {count_same_mission(df_interventions_1_comb)} rows\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11436\\1231017698.py\u001b[0m in \u001b[0;36mcombine_intervention_rows\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# Group by 'mission_id' and apply the aggregation rules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mdf_combined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mission_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maggregation_rules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf_combined\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGroupByApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 869\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36magg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;31m# we require a list, but not a 'str'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36magg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[1;31m# key used for column selection and output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m             results = {\n\u001b[0m\u001b[0;32m    482\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m             }\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[1;31m# key used for column selection and output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m             results = {\n\u001b[1;32m--> 482\u001b[1;33m                 \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m             }\n\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_agg_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m                 \u001b[1;31m# TODO: KeyError is raised in _python_agg_general,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_python_agg_general\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1488\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1489\u001b[0m                 \u001b[1;31m# if this function is invalid for this dtype, we will ignore it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1490\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1491\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1492\u001b[0m                 \u001b[0mwarn_dropping_nuisance_columns_deprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"agg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py\u001b[0m in \u001b[0;36magg_series\u001b[1;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_aggregate_series_pure_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m         \u001b[0mnpvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_convert_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtry_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py\u001b[0m in \u001b[0;36m_aggregate_series_pure_python\u001b[1;34m(self, obj, func)\u001b[0m\n\u001b[0;32m   1003\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplitter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m             \u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"groupby\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1005\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1474\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_python_agg_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1475\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_builtin_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1476\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m         \u001b[1;31m# iterate through \"columns\" ex exclusions to populate output dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11436\\1231017698.py\u001b[0m in \u001b[0;36mget_max_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     22\u001b[0m                     \u001b[1;31m# Default case if format is unknown\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mmax_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mconvert_to_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmax_value\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11436\\1231017698.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m     22\u001b[0m                     \u001b[1;31m# Default case if format is unknown\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mmax_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mconvert_to_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmax_value\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11436\\1231017698.py\u001b[0m in \u001b[0;36mconvert_to_float\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;31m# Handle time or duration strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                 \u001b[0mparts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                     \u001b[1;31m# Assuming format is HH:MM:SS.SSS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "df_interventions_1_comb= combine_intervention_rows(df_internetions_1)\n",
    "df_interventions_2_comb= combine_intervention_rows(df_internetions_2)\n",
    "df_interventions_3_comb= combine_intervention_rows(df_internetions_3)\n",
    "df_interventions_bxl= combine_intervention_rows(df_interventions_bxl)\n",
    "df_interventions_bxl2= combine_intervention_rows(df_interventions_bxl)\n",
    "print(f\"Same mission ID in intervention_1 for {count_same_mission(df_interventions_1_comb)} rows\")\n",
    "print(f\"Same mission ID in intervention_2 for {count_same_mission(df_interventions_2_comb)} rows\")\n",
    "print(f\"Same mission ID in intervention_3 for {count_same_mission(df_interventions_3_comb)} rows\")\n",
    "print(f\"Same mission ID in intervention_bxl for {count_same_mission(df_interventions_bxl)} rows\")\n",
    "print(f\"Same mission ID in intervention_bxl2 for {count_same_mission(df_interventions_bxl2)} rows\")\n",
    "df_combined_interventions = pd.concat([df_interventions_1_comb, df_interventions_2_comb, df_interventions_3_comb,df_interventions_bxl,df_interventions_bxl2], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of data before imputing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Percentage Missing (%)</th>\n",
       "      <th>Data Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>postalcode_intervention</td>\n",
       "      <td>57656</td>\n",
       "      <td>73.725129</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>calculated_distance_destination</td>\n",
       "      <td>39432</td>\n",
       "      <td>50.421973</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5</td>\n",
       "      <td>30677</td>\n",
       "      <td>39.226894</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t6</td>\n",
       "      <td>27424</td>\n",
       "      <td>35.067260</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t4</td>\n",
       "      <td>27015</td>\n",
       "      <td>34.544269</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>number_of_transported_persons</td>\n",
       "      <td>26782</td>\n",
       "      <td>34.246330</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>intervention_time_(t1confirmed)</td>\n",
       "      <td>25635</td>\n",
       "      <td>32.779653</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>departure_time_(t1confirmed)</td>\n",
       "      <td>23808</td>\n",
       "      <td>30.443456</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>calculated_traveltime_destinatio</td>\n",
       "      <td>23798</td>\n",
       "      <td>30.430669</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>date</td>\n",
       "      <td>20522</td>\n",
       "      <td>26.241624</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>intervention_duration</td>\n",
       "      <td>19474</td>\n",
       "      <td>24.901540</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>t7</td>\n",
       "      <td>17682</td>\n",
       "      <td>22.610097</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>t3</td>\n",
       "      <td>15742</td>\n",
       "      <td>20.129405</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>t2</td>\n",
       "      <td>11932</td>\n",
       "      <td>15.257532</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>latitude_permanence</td>\n",
       "      <td>9042</td>\n",
       "      <td>11.562068</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>latitude_intervention</td>\n",
       "      <td>8875</td>\n",
       "      <td>11.348524</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>waiting_time</td>\n",
       "      <td>6419</td>\n",
       "      <td>8.208020</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>longitude_permanence</td>\n",
       "      <td>692</td>\n",
       "      <td>0.884865</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>postalcode_permanence</td>\n",
       "      <td>177</td>\n",
       "      <td>0.226331</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>longitude_intervention</td>\n",
       "      <td>49</td>\n",
       "      <td>0.062657</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>streetname_permanence</td>\n",
       "      <td>14</td>\n",
       "      <td>0.017902</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cityname_intervention</td>\n",
       "      <td>7</td>\n",
       "      <td>0.008951</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>t1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>vector_type</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>t0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mission_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Column  Missing Values  Percentage Missing (%)  \\\n",
       "0            postalcode_intervention           57656               73.725129   \n",
       "1    calculated_distance_destination           39432               50.421973   \n",
       "2                                 t5           30677               39.226894   \n",
       "3                                 t6           27424               35.067260   \n",
       "4                                 t4           27015               34.544269   \n",
       "5      number_of_transported_persons           26782               34.246330   \n",
       "6    intervention_time_(t1confirmed)           25635               32.779653   \n",
       "7       departure_time_(t1confirmed)           23808               30.443456   \n",
       "8   calculated_traveltime_destinatio           23798               30.430669   \n",
       "9                               date           20522               26.241624   \n",
       "10             intervention_duration           19474               24.901540   \n",
       "11                                t7           17682               22.610097   \n",
       "12                                t3           15742               20.129405   \n",
       "13                                t2           11932               15.257532   \n",
       "14               latitude_permanence            9042               11.562068   \n",
       "15             latitude_intervention            8875               11.348524   \n",
       "16                      waiting_time            6419                8.208020   \n",
       "17              longitude_permanence             692                0.884865   \n",
       "18             postalcode_permanence             177                0.226331   \n",
       "19            longitude_intervention              49                0.062657   \n",
       "20             streetname_permanence              14                0.017902   \n",
       "21             cityname_intervention               7                0.008951   \n",
       "22                                t1               0                0.000000   \n",
       "23                       vector_type               0                0.000000   \n",
       "24                                t0               0                0.000000   \n",
       "25                        mission_id               0                0.000000   \n",
       "\n",
       "   Data Type  \n",
       "0     object  \n",
       "1    float64  \n",
       "2     object  \n",
       "3     object  \n",
       "4     object  \n",
       "5    float64  \n",
       "6    float64  \n",
       "7    float64  \n",
       "8    float64  \n",
       "9     object  \n",
       "10   float64  \n",
       "11    object  \n",
       "12    object  \n",
       "13    object  \n",
       "14   float64  \n",
       "15   float64  \n",
       "16   float64  \n",
       "17   float64  \n",
       "18   float64  \n",
       "19   float64  \n",
       "20    object  \n",
       "21    object  \n",
       "22    object  \n",
       "23    object  \n",
       "24    object  \n",
       "25     int64  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_dataframe(df_combined_interventions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- number_of_transported_persons -> mean impute \n",
    "- calculated_traveltime_destinatio -> regression impute \n",
    "- calculated_distance_destination -> regression impute \n",
    "- t's -> use time between and devide by coumns needed\n",
    "- intervention_duration -> calculate from t's?  \n",
    "- longitude_intervention + latitude_permanence -> get from adress \n",
    "- longitude_intervention + latitude_intervention -> get from adres?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute the numbers of transported persons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimpleImputer object with strategy set to 'mean'\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "number_of_transported = df_combined_interventions['number_of_transported_persons'].values.reshape(-1, 1)\n",
    "df_combined_interventions['number_of_transported_persons'] = imputer.fit_transform(number_of_transported)\n",
    "df_combined_interventions['number_of_transported_persons']=round(df_combined_interventions['number_of_transported_persons'],0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute the T columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_timestamps(df):\n",
    "    def to_seconds(time_str):\n",
    "        \"\"\"Convert HH:MM:SS or full datetime to total seconds.\"\"\"\n",
    "        if pd.isna(time_str):\n",
    "            return None\n",
    "        # Check if the string includes a date part and extract only the time part if so\n",
    "        if ' ' in time_str:\n",
    "            time_str = time_str.split(' ')[-1]  # Assume the last part is time\n",
    "        # Split the time part into hours, minutes, and seconds\n",
    "        parts = time_str.split(':')\n",
    "        h, m = int(parts[0]), int(parts[1])\n",
    "        s = int(parts[2]) if len(parts) == 3 else 0  # Handle cases without seconds\n",
    "        return h * 3600 + m * 60 + s\n",
    "\n",
    "    def to_hms(seconds):\n",
    "        \"\"\"Convert total seconds back to HH:MM:SS.\"\"\"\n",
    "        if seconds is None:\n",
    "            return None\n",
    "        h = seconds // 3600\n",
    "        m = (seconds % 3600) // 60\n",
    "        s = seconds % 60\n",
    "        return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "    # Iterate over each row\n",
    "    for index, row in df.iterrows():\n",
    "        timestamps = row[['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']]\n",
    "        missing_indices = [i for i, x in enumerate(timestamps) if pd.isna(x)]\n",
    "        if not missing_indices:\n",
    "            continue\n",
    "        \n",
    "        for missing_index in missing_indices:\n",
    "            if missing_index == 0 or missing_index == len(timestamps) - 1:\n",
    "                continue\n",
    "            prev_known_index = next((i for i in range(missing_index - 1, -1, -1) if not pd.isna(timestamps[i])), None)\n",
    "            next_known_index = next((i for i in range(missing_index + 1, len(timestamps)) if not pd.isna(timestamps[i])), None)\n",
    "            \n",
    "            if prev_known_index is None or next_known_index is None:\n",
    "                continue\n",
    "            \n",
    "            start_time = to_seconds(timestamps.iloc[prev_known_index])\n",
    "            end_time = to_seconds(timestamps.iloc[next_known_index])\n",
    "            if start_time is not None and end_time is not None:\n",
    "                interval = (end_time - start_time) // (next_known_index - prev_known_index)\n",
    "                \n",
    "                for i in range(prev_known_index + 1, next_known_index):\n",
    "                    start_time += interval\n",
    "                    df.at[index, f't{i}'] = to_hms(start_time)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_interventions = impute_timestamps(df_combined_interventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Percentage Missing (%)</th>\n",
       "      <th>Data Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>postalcode_intervention</td>\n",
       "      <td>57656</td>\n",
       "      <td>73.725129</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>calculated_distance_destination</td>\n",
       "      <td>39432</td>\n",
       "      <td>50.421973</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intervention_time_(t1confirmed)</td>\n",
       "      <td>25635</td>\n",
       "      <td>32.779653</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>departure_time_(t1confirmed)</td>\n",
       "      <td>23808</td>\n",
       "      <td>30.443456</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>calculated_traveltime_destinatio</td>\n",
       "      <td>23798</td>\n",
       "      <td>30.430669</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>date</td>\n",
       "      <td>20522</td>\n",
       "      <td>26.241624</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>intervention_duration</td>\n",
       "      <td>19474</td>\n",
       "      <td>24.901540</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>t7</td>\n",
       "      <td>17682</td>\n",
       "      <td>22.610097</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>t6</td>\n",
       "      <td>10407</td>\n",
       "      <td>13.307503</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>t5</td>\n",
       "      <td>9876</td>\n",
       "      <td>12.628510</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>t4</td>\n",
       "      <td>9875</td>\n",
       "      <td>12.627231</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>t3</td>\n",
       "      <td>9871</td>\n",
       "      <td>12.622117</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>t2</td>\n",
       "      <td>9861</td>\n",
       "      <td>12.609329</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>latitude_permanence</td>\n",
       "      <td>9042</td>\n",
       "      <td>11.562068</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>latitude_intervention</td>\n",
       "      <td>8875</td>\n",
       "      <td>11.348524</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>waiting_time</td>\n",
       "      <td>6419</td>\n",
       "      <td>8.208020</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>longitude_permanence</td>\n",
       "      <td>692</td>\n",
       "      <td>0.884865</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>postalcode_permanence</td>\n",
       "      <td>177</td>\n",
       "      <td>0.226331</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>longitude_intervention</td>\n",
       "      <td>49</td>\n",
       "      <td>0.062657</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>streetname_permanence</td>\n",
       "      <td>14</td>\n",
       "      <td>0.017902</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cityname_intervention</td>\n",
       "      <td>7</td>\n",
       "      <td>0.008951</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mission_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>t1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>vector_type</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>t0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>number_of_transported_persons</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Column  Missing Values  Percentage Missing (%)  \\\n",
       "0            postalcode_intervention           57656               73.725129   \n",
       "1    calculated_distance_destination           39432               50.421973   \n",
       "2    intervention_time_(t1confirmed)           25635               32.779653   \n",
       "3       departure_time_(t1confirmed)           23808               30.443456   \n",
       "4   calculated_traveltime_destinatio           23798               30.430669   \n",
       "5                               date           20522               26.241624   \n",
       "6              intervention_duration           19474               24.901540   \n",
       "7                                 t7           17682               22.610097   \n",
       "8                                 t6           10407               13.307503   \n",
       "9                                 t5            9876               12.628510   \n",
       "10                                t4            9875               12.627231   \n",
       "11                                t3            9871               12.622117   \n",
       "12                                t2            9861               12.609329   \n",
       "13               latitude_permanence            9042               11.562068   \n",
       "14             latitude_intervention            8875               11.348524   \n",
       "15                      waiting_time            6419                8.208020   \n",
       "16              longitude_permanence             692                0.884865   \n",
       "17             postalcode_permanence             177                0.226331   \n",
       "18            longitude_intervention              49                0.062657   \n",
       "19             streetname_permanence              14                0.017902   \n",
       "20             cityname_intervention               7                0.008951   \n",
       "21                        mission_id               0                0.000000   \n",
       "22                                t1               0                0.000000   \n",
       "23                       vector_type               0                0.000000   \n",
       "24                                t0               0                0.000000   \n",
       "25     number_of_transported_persons               0                0.000000   \n",
       "\n",
       "   Data Type  \n",
       "0     object  \n",
       "1    float64  \n",
       "2    float64  \n",
       "3    float64  \n",
       "4    float64  \n",
       "5     object  \n",
       "6    float64  \n",
       "7     object  \n",
       "8     object  \n",
       "9     object  \n",
       "10    object  \n",
       "11    object  \n",
       "12    object  \n",
       "13   float64  \n",
       "14   float64  \n",
       "15   float64  \n",
       "16   float64  \n",
       "17   float64  \n",
       "18   float64  \n",
       "19    object  \n",
       "20    object  \n",
       "21     int64  \n",
       "22    object  \n",
       "23    object  \n",
       "24    object  \n",
       "25   float64  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_dataframe(df_combined_interventions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute the intervention coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_coordinates(row):\n",
    "    # Check if both latitude and longitude are missing\n",
    "    if pd.isnull(row['latitude_intervention']) or pd.isnull(row['longitude_intervention']):\n",
    "        try:\n",
    "            # Attempt to fetch location using city name\n",
    "            location = geolocator.geocode(row['cityname_intervention'])\n",
    "            if location:\n",
    "                return location.latitude, location.longitude\n",
    "            else:\n",
    "                return None, None\n",
    "        except (GeocoderTimedOut, GeocoderServiceError) as e:\n",
    "            print(f\"Geocoding error for {row['cityname_intervention']}: {e}\")\n",
    "            return None, None\n",
    "    else:\n",
    "        # Return existing coordinates\n",
    "        return row['latitude_intervention'], row['longitude_intervention']\n",
    "    \n",
    "df_combined_interventions[['latitude_intervention', 'longitude_intervention']] = df_combined_interventions.apply(fetch_coordinates, axis=1, result_type='expand')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
